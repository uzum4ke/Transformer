{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Hyperparameters\n",
    "block_size = 8\n",
    "n_embd = 32\n",
    "batch_size = 32\n",
    "learning_rate = 1e-3\n",
    "max_iters = 100  # Reduced for demonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_size, head_size, causal=False):\n",
    "        super(Attention, self).__init__()\n",
    "        self.key = nn.Linear(feature_size, head_size, bias=False)  # Transform for keys\n",
    "        self.query = nn.Linear(feature_size, head_size, bias=False)  # Transform for queries\n",
    "        self.value = nn.Linear(feature_size, head_size, bias=False)  # Transform for values\n",
    "        self.scale = head_size ** -0.5  # Scaling factor to stabilize training\n",
    "        self.causal = causal  # Toggle to enable causal attention\n",
    "        self.register_buffer(\"causal_mask\", None)  # Register buffer for causal mask\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \n",
    "        batch_size, seq_length, _ = x.shape\n",
    "        k = self.key(x)  # Compute keys\n",
    "        q = self.query(x)  # Compute queries\n",
    "        v = self.value(x)  # Compute values\n",
    "\n",
    "        # Compute attention scores using scaled dot-product\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Apply causal mask to prevent attention to future positions in sequence\n",
    "        if self.causal:\n",
    "            if self.causal_mask is None or self.causal_mask.size(0) != seq_length:\n",
    "                # Create a lower triangular matrix that allows attending to earlier positions only\n",
    "                causal_mask = torch.tril(torch.ones((seq_length, seq_length), device=x.device)).bool()\n",
    "                self.register_buffer('causal_mask', causal_mask)\n",
    "            scores = scores.masked_fill(~self.causal_mask, float('-inf'))  # Apply the causal mask\n",
    "\n",
    "        # Apply additional mask provided by the user (e.g., for ignoring padding)\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).expand_as(scores)  # Adjust mask dimensions and apply\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # Compute attention weights using softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Compute weighted sum of values based on attention weights\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size, feature_size):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        # Initialize several independent attention heads\n",
    "        self.heads = nn.ModuleList([\n",
    "            Attention(feature_size, head_size, causal=False) for _ in range(num_heads)\n",
    "        ])\n",
    "        # Output linear layer to combine and project the concatenated outputs of attention heads back to the original feature size\n",
    "        self.output_projection = nn.Linear(num_heads * head_size, feature_size)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \n",
    "        # Collect outputs from each attention head, passing the optional mask if provided\n",
    "        head_outputs = [head(x, mask) for head in self.heads]  # List of tensors from each head\n",
    "\n",
    "        # Concatenate the outputs of all attention heads along the last dimension\n",
    "        concatenated = torch.cat(head_outputs, dim=-1)\n",
    "\n",
    "        # Project the concatenated outputs back to the original feature size\n",
    "        return self.output_projection(concatenated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embd, expansion_factor=4, dropout_rate=0.1):\n",
    "        super(FeedForward, self).__init__()\n",
    "        # Sequential container to execute a series of operations that constitute a feedforward network\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, n_embd * expansion_factor),  # First linear layer to expand feature space\n",
    "            nn.ReLU(),  # Non-linear activation function to introduce non-linearity between linear transformations\n",
    "            nn.Dropout(dropout_rate),  # Dropout for regularization\n",
    "            nn.Linear(n_embd * expansion_factor, n_embd)  # Second linear layer to project back to original feature size\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input x through the sequential network\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, n_embd, block_size, forecast_horizon):\n",
    "        super().__init__()\n",
    "        self.forecast_horizon = forecast_horizon\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)  # Positional embeddings to encode sequence position info\n",
    "        self.input_projection = nn.Linear(1, n_embd)  # Linear transformation to project input data to embedding dimension\n",
    "        self.sa_heads = MultiHeadAttention(4, n_embd//4, n_embd)  # Multi-head attention mechanism\n",
    "        self.ffwd = FeedForward(n_embd)  # Feedforward neural network layer\n",
    "        self.output_projection = nn.Linear(n_embd, forecast_horizon)  # Final linear layer to output predictions for each forecast step\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T = x.shape  # Batch size and sequence length\n",
    "        device = x.device  # Device to ensure compatibility of tensors and operations\n",
    "\n",
    "        # Transform scalar inputs to high-dimensional embeddings\n",
    "        x = self.input_projection(x.view(B * T, 1)).view(B, T, -1)\n",
    "        \n",
    "        # Add positional embeddings to provide context about the position in the sequence\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))\n",
    "        x = x + pos_emb.unsqueeze(0)  # Broadcast add across the batch\n",
    "\n",
    "        # Apply self-attention across the sequence\n",
    "        x = self.sa_heads(x)\n",
    "        \n",
    "        # Apply a feedforward network\n",
    "        x = self.ffwd(x)\n",
    "        \n",
    "        # Project the embeddings to forecast horizon outputs\n",
    "        output = self.output_projection(x)\n",
    "        \n",
    "        # Reshape output to ensure each batch predicts exactly forecast_horizon future steps\n",
    "        return output[:, -1, :].view(B, self.forecast_horizon)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
